# Top 20 Important Keras Interview Questions â€” 4MH23CS042

---

## 1. What is Keras and why is it preferred over raw TensorFlow?
**Keras** is a high-level API that runs on top of TensorFlow. It is preferred because it is **user-friendly, modular, and easy to extend**, allowing for rapid prototyping. While TensorFlow handles low-level tensor operations, Keras abstracts them into building blocks (Layers, Models) for easier deep learning development.

## 2. Sequential API vs. Functional API: When to use which?
- **Sequential API**: Use for simple, linear stacks of layers (one input, one output).
- **Functional API**: Use for complex architectures like **ResNets, Inception networks, or models with shared layers, multi-inputs, or multi-outputs**.

## 3. Explain the `compile()` method. What are its arguments?
`compile()` configures the model for training.
- **Optimizer**: How weights are updated (e.g., `Adam`, `SGD`).
- **Loss**: The error function to minimize (e.g., `categorical_crossentropy`, `mse`).
- **Metrics**: Performance measures to monitor (e.g., `accuracy`).

## 4. What is the difference between `loss` and `metrics`?
- **Loss**: Use by the **optimizer** to update weights during training (must be differentiable).
- **Metrics**: Used **only for evaluation** and human monitoring; they do not affect training updates.

## 5. What are Callbacks? Give examples.
Callbacks are functions used to customize the training process at specific stages (e.g., end of epoch). Important ones:
- **`EarlyStopping`**: Stops training if validation loss stops improving.
- **`ModelCheckpoint`**: Saves the best model during training.
- **`ReduceLROnPlateau`**: Lowers learning rate if training plateaus.

## 6. How do you handle overfitting in Keras?
- **Dropout**: Randomly disables neurons during training.
- **Regularization (L1/L2)**: Penalizes large weights.
- **Early Stopping**: Stops training before the model memorizes noise.
- **Data Augmentation**: Increases training data variety.

## 7. What is the role of the Activation Function?
It introduces **non-linearity** to the network, allowing it to learn complex patterns. Without it, a neural network is just a linear regression model.
- **Hidden Layers**: ReLU (prevents vanishing gradient).
- **Output Layer**: Sigmoid (Binary), Softmax (Multi-class), Linear (Regression).

## 8. Explain `padding='valid'` vs `padding='same'` in Conv2D layers.
- **`valid`**: No padding. The output size shrinks (Input < Kernel implies data loss).
- **`same`**: Adds zeros around input so **Output Size = Input Size**.

## 9. What is Batch Normalization?
It normalizes layer inputs to have mean 0 and variance 1.
**Benefits**: Faster convergence, allows higher learning rates, and reduces sensitivity to weight initialization.

## 10. `model.fit()` vs `model.predict()` vs `model.evaluate()`
- **`model.fit()`**: Trains the model (updates weights).
- **`model.evaluate()`**: Tests the model (returns loss & metrics).
- **`model.predict()`**: Uses the model (returns raw output predictions).

## 11. What is Transfer Learning?
Using a pre-trained model (like VGG16, ResNet trained on ImageNet) on a new, smaller dataset.
**Steps**: Load base model $\rightarrow$ Freeze layers $\rightarrow$ Add custom classifier $\rightarrow$ Train.

## 12. What is the difference between specific Loss Functions?
- **MSE (Mean Squared Error)**: Regression problems.
- **Binary Crossentropy**: Classification with 2 classes.
- **Categorical Crossentropy**: Classification with >2 classes (One-hot encoded labels).
- **Sparse Categorical Crossentropy**: Classification with >2 classes (Integer labels).

## 13. What is the Vanishing Gradient Problem? How does Keras solve it?
In deep networks, gradients become too small for effective weight updates.
**Solution**: Use **ReLU** activation (instead of Sigmoid/Tanh) and **Batch Normalization**.

## 14. What does `Flatten()` do?
Converts multi-dimensional feature maps (from Conv/Pool layers) into a **1D vector** to feed into Dense (Fully Connected) layers.

## 15. What are Optimizers? Comparing SGD vs. Adam.
Optimizers update weights to minimize loss.
- **SGD**: Updates based on small batches; can oscillate.
- **Adam**: Adaptive learning rate; generally **converges faster** and is the default choice for most problems.

## 16. What is the difference between `Batch Size` and `Epoch`?
- **Batch Size**: Number of samples processed before **one weight update**.
- **Epoch**: One complete pass through the **entire dataset**.

## 17. How does MaxPooling work?
It downsamples the input by taking the **maximum value** in a defined window (e.g., 2x2).
**Purpose**: Reduces dimensions/computation and extracts dominant features (e.g., edges).

## 18. What is Data Augmentation?
Generating new training samples from existing ones by applying random transformations (rotation, zoom, flip) using `ImageDataGenerator`. It helps prevent overfitting.

## 19. How do you save and load models?
- **Save**: `model.save('model.h5')` (Saves architecture + weights + optimizer).
- **Load**: `load_model('model.h5')`.

## 20. Why do we need `input_shape` only in the first layer?
The first layer needs to know the shape of the input data to build the weight matrix. Subsequent layers automatically infer their input shape from the previous layer's output.
