# ğŸ¤– LLM Interview Prep Guide: T5 | LLaMA | Falcon

> A quick reference guide covering model briefs, comparisons, and 15 basic interview questions.

---

## ğŸ“Œ Table of Contents

- [T5 â€” Text-To-Text Transfer Transformer](#-t5--text-to-text-transfer-transformer)
- [LLaMA â€” Large Language Model Meta AI](#-llama--large-language-model-meta-ai)
- [Falcon â€” Fast and Lightweight LLM](#-falcon--fast-and-lightweight-llm)
- [Quick Comparison Table](#-quick-comparison-table)
- [15 Basic Interview Questions](#-15-basic-interview-questions)

---

## ğŸ”µ T5 â€” Text-To-Text Transfer Transformer

**Creator:** Google Research (2019)

T5 treats **every NLP task as a text-to-text problem**. Whether it's translation, summarization, or classification â€” all tasks are framed as: `input text â†’ output text`.

### ğŸ“ Where Is It Used?
- Text summarization
- Language translation
- Question answering
- Sentiment analysis
- Chatbots and NLP pipelines

### âœ… Pros & âŒ Cons

| âœ… Pros | âŒ Cons |
|---|---|
| Unified framework for all NLP tasks | Computationally heavy |
| Highly versatile | Requires large memory |
| Strong benchmark performance | Not ideal for real-time apps |

### ğŸ“Š Accuracy
Achieved state-of-the-art on **GLUE** and **SuperGLUE** benchmarks. ~91%+ on many classification tasks.

### ğŸ—ï¸ Architecture
```
Input Text â†’ [Encoder] â†’ Context Representation â†’ [Decoder] â†’ Output Text
Task Prefix: "summarize: ...", "translate English to French: ..."
```

---

## ğŸŸ  LLaMA â€” Large Language Model Meta AI

**Creator:** Meta AI (2023), available as LLaMA 1, 2, and 3

LLaMA is Meta's **open-source LLM** designed to be efficient and accessible â€” delivering GPT-level performance at smaller scales.

### ğŸ“ Where Is It Used?
- Chatbots and virtual assistants
- Code generation
- Research and fine-tuning
- Domain-specific AI apps (medical, legal, etc.)

### âœ… Pros & âŒ Cons

| âœ… Pros | âŒ Cons |
|---|---|
| Open-source and free to use | Needs powerful GPU to run locally |
| Highly fine-tunable | Can hallucinate facts |
| Efficient â€” strong performance per parameter | Smaller variants are less capable |

### ğŸ“Š Accuracy
LLaMA 3 (70B) scores ~**85%+ on MMLU** â€” competitive with GPT-4 on many benchmarks.

### ğŸ”„ Version Differences
```
LLaMA 1 â†’ Research-only release, 7Bâ€“65B params
LLaMA 2 â†’ Open for commercial use, improved safety
LLaMA 3 â†’ Better reasoning, longer context, stronger benchmarks
```

---

## ğŸŸ¢ Falcon â€” Fast and Lightweight LLM

**Creator:** Technology Innovation Institute (TII), UAE (2023)

Falcon is a high-performance **open-source LLM** built for efficiency. Falcon-40B was the top open-source model on Hugging Face's Open LLM Leaderboard when released.

### ğŸ“ Where Is It Used?
- Text generation
- Summarization
- Enterprise AI applications
- Research and production deployments

### âœ… Pros & âŒ Cons

| âœ… Pros | âŒ Cons |
|---|---|
| Top open-source benchmark performance | Smaller community vs LLaMA |
| Apache 2.0 license (commercial use OK) | Limited multilingual support |
| Fast inference | Fewer fine-tuned variants available |

### ğŸ“Š Accuracy
- Falcon-40B: ~**79% on MMLU**
- Falcon-180B: Rivals GPT-3.5 on multiple benchmarks

---

## ğŸ“Š Quick Comparison Table

| Model  | Maker  | Open Source | Best For           | Size Range  | License     |
|--------|--------|-------------|---------------------|-------------|-------------|
| T5     | Google | âœ… Yes      | NLP tasks (S2S)    | 60M â€“ 11B   | Apache 2.0  |
| LLaMA  | Meta   | âœ… Yes      | General LLM tasks  | 7B â€“ 70B+   | Custom Meta |
| Falcon | TII    | âœ… Yes      | Text generation    | 7B â€“ 180B   | Apache 2.0  |

---

## ğŸ¯ 15 Basic Interview Questions

### T5 Questions

**1. What does T5 stand for, and what makes it unique?**
> T5 = Text-To-Text Transfer Transformer. It converts every NLP task into a text-in, text-out format using a single unified model.

**2. What is the "text-to-text" framework in T5?**
> Instead of separate models per task, T5 uses one model with task prefixes like `"summarize: ..."` or `"translate English to French: ..."`.

**3. What dataset was T5 pre-trained on?**
> The **C4 (Colossal Clean Crawled Corpus)** dataset â€” a massive cleaned web text dataset.

**4. Name two tasks T5 can perform.**
> Translation and summarization. It can also do Q&A, sentiment analysis, and more.

**5. What architecture does T5 use?**
> **Encoder-Decoder Transformer** architecture â€” encoder processes input, decoder generates output.

---

### LLaMA Questions

**6. What is LLaMA and who created it?**
> LLaMA is Meta's open-source large language model family, designed for research and commercial use with efficient performance.

**7. Why is LLaMA popular among researchers?**
> It's open-source, highly customizable, and delivers strong performance even at smaller parameter sizes.

**8. What is fine-tuning and why is it used with LLaMA?**
> Fine-tuning adapts LLaMA on domain-specific data (medical, legal, etc.) without training from scratch, saving cost and time.

**9. What is the difference between LLaMA 1, 2, and 3?**
> Each version improved performance, context window length, and safety alignment. LLaMA 3 is the most capable and aligned version.

**10. What does "hallucination" mean in LLMs like LLaMA?**
> When the model confidently generates incorrect or made-up information â€” a common challenge in all LLMs.

---

### Falcon Questions

**11. What is Falcon and who built it?**
> Falcon is an open-source LLM by the **Technology Innovation Institute (TII)** in Abu Dhabi, UAE.

**12. What makes Falcon stand out from other open-source models?**
> It was the first open-source model to top the Hugging Face Open LLM Leaderboard, with a fully commercial Apache 2.0 license.

**13. What is MMLU and how does Falcon perform on it?**
> MMLU (Massive Multitask Language Understanding) is a standard LLM benchmark. Falcon-40B scores ~79%, and Falcon-180B rivals GPT-3.5.

**14. What license does Falcon use?**
> **Apache 2.0** â€” completely free for both research and commercial use.

**15. Compare Falcon and LLaMA in one sentence.**
> Both are open-source LLMs, but LLaMA has a larger community and more fine-tuned variants, while Falcon was built for raw performance and commercial accessibility.

---

## ğŸ“ Key Takeaways

- **T5** â†’ Best for structured NLP tasks using encoder-decoder design
- **LLaMA** â†’ Best for research, fine-tuning, and general-purpose generation
- **Falcon** â†’ Best for commercial production with top leaderboard performance

---

*Prepared for interview prep | Topics: T5 Â· LLaMA Â· Falcon*